{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The Tensor objects",
   "id": "f8ada76ae37236c4"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pixelprism.math as pm\n",
    "import numpy as np"
   ],
   "id": "9fef73bbb700c7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creation\n\nA `Tensor` is PixelPrism's friendly wrapper around NumPy arrays. It keeps the data, the shape, and the dtype in one tidy place, while staying easy to construct in a bunch of ways. Let's tour every creation path in `pixelprism/math/tensor.py`.",
   "id": "70800328a2bb4adc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1) Direct constructor: scalars, vectors, matrices\n\nThe `Tensor` constructor accepts numbers, lists, nested lists, and NumPy arrays. Shapes are inferred automatically.",
   "id": "d40563a74ada45bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scalar = pm.Tensor(3)\nscalar\n",
   "id": "89c487ffa56f4669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vector = pm.Tensor([1, 2, 3])\nmatrix = pm.Tensor([[1, 2], [3, 4]])\nvector, matrix\n",
   "id": "bd0678e1238c4f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A single number creates a rank-0 tensor (a scalar). A 1D list becomes a vector, and nested lists become matrices or higher-rank tensors.",
   "id": "8b89f7f05d894fe0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) From NumPy arrays\n\nIf you already have NumPy data, pass it directly. NumPy scalars also work.",
   "id": "be8795ec92a64bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np_vec = np.array([10, 20, 30], dtype=np.int32)\nnp_scalar = np.array(2.5)\n\nfrom_np_vec = pm.Tensor(np_vec)\nfrom_np_scalar = pm.Tensor(np_scalar)\nfrom_np_vec, from_np_scalar\n",
   "id": "c5482675f280443b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3) Static helpers for clarity\n\nThese are thin wrappers around the constructor, but they are explicit and readable in tutorials or pipelines.",
   "id": "00cdbc7dfeb840bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explicit_np = pm.Tensor.from_numpy(np.arange(6).reshape(2, 3))\n",
    "print(explicit_np)\n"
   ],
   "id": "513e695c4e254715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explicit_list = pm.Tensor.from_list([1, 2, 3], dtype=pm.Z)\n",
    "explicit_list\n"
   ],
   "id": "e17f8f34cb5949e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`from_list` requires a `DType` so there is no ambiguity about integers vs floats.",
   "id": "4d30e39c1e8245c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4) Zeros factory\n\n`Tensor.zeros` creates a tensor filled with zeros. The shape can be an int, a tuple, a list, or a `TensorShape`.",
   "id": "d74f02e9eba74b07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "zeros_tuple = pm.t_zeros((2, 3))\n",
    "zeros_int = pm.t_zeros(4)\n",
    "zeros_shape = pm.t_zeros(pm.ts_matrix(2, 2))\n",
    "zeros_tuple, zeros_int, zeros_shape\n"
   ],
   "id": "684cfc54b3144a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5) Dtype and mutability knobs\n\nThe constructor accepts `dtype` and `mutable`. `dtype` can be a `pm.DType`, a NumPy dtype, or a Python type (`int`, `float`, `bool`, `complex`). `mutable=False` makes the tensor read-only for in-place ops.",
   "id": "93830da636d24f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "float_tensor = pm.Tensor([1, 2, 3], dtype=pm.DType.R)\n",
    "bool_tensor = pm.Tensor([1, 0, 1], dtype=bool)\n",
    "immutable = pm.Tensor([1, 2, 3], mutable=False)\n",
    "float_tensor, bool_tensor, immutable\n"
   ],
   "id": "4b962e3fca0447d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Algebra\n\nTensors are made for algebra. Every operation below is **elementwise**: each number talks only to its matching partner. This mirrors how we build models in machine learning: data flows through layers as a chain of simple, reliable transforms.",
   "id": "fa994db6c917463b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1) The four basics: +, -, *, /\n\nThink of these as the arithmetic gears of a neural network: add a bias, scale activations, normalize, or compute residuals.",
   "id": "ba78dde6cffc4f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x = pm.Tensor([1.0, 2.0, 3.0])\ny = pm.Tensor([10.0, 20.0, 30.0])\n\nx_plus_y = x + y\nx_minus_y = x - y\nx_times_y = x * y\nx_div_y = x / y\n\nx_plus_y, x_minus_y, x_times_y, x_div_y\n",
   "id": "b59ab9f58aa1455b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also mix tensors with scalars. Broadcasting applies the scalar across every element.",
   "id": "e7f79a731f4f4518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shift = x + 0.5\nscale = x * 2\nnormalize = x / 10\nshift, scale, normalize\n",
   "id": "b9e5bb4c02d24f5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also mix tensors with Numpy array. Broadcasting applies the scalar across every element.",
   "id": "386204af6c03d6d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shift = x + np.array(0.5)\n",
    "shift_vector = x + np.array([0.5, 0.5, 0.5])\n",
    "scale = x * np.array(2)\n",
    "normalize = x / np.array([10, 10, 10])\n",
    "shift, scale, normalize"
   ],
   "id": "5d971b7fed3d3106",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) Power moves: `**`, `pow`, and friends\n\nIn ML, exponents show up in loss functions (squared error), normalization, and feature engineering.",
   "id": "39dee7b1c76c4ddf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "squared_op = x ** 2\nsquared_method = x.pow(2)\n\ncube = x ** 3\nfractional = x ** 0.5\n\nsquared_op, squared_method, cube, fractional\n",
   "id": "bbb62eee6c814060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`**` and `pow` are equivalent; choose the style that reads best in your notebook.",
   "id": "997c2d8fe052435e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3) Convenience helpers: `square()` and `cbrt()`\n\nThese are readable shortcuts for common transforms. `square()` is the workhorse of least-squares losses, while `cbrt()` is a gentler nonlinearity.",
   "id": "b7cae4f8982c4daf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "square = x.square()\n",
    "root3 = x.cbrt()\n",
    "\n",
    "square, root3\n"
   ],
   "id": "46e0531a0fea4243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4) Advanced unary operations\n\nThese functions show up everywhere in mathematics and machine learning: exponentials and logs drive probability and optimization, while absolute values underpin L1 regularization and robust losses.",
   "id": "35f160a344e6435c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We keep inputs in safe domains (e.g., positive values for logs) to avoid undefined results.",
   "id": "9a0fc8eb38924ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "u = pm.Tensor([0.1, 0.5, 1.0])\n\nrecip = u.reciprocal()  # 1/x\nexp_u = u.exp()\nexp2_u = u.exp2()\nexpm1_u = u.expm1()\n\nrecip, exp_u, exp2_u, expm1_u\n",
   "id": "34633761ee224454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "v = pm.Tensor([1.0, 2.0, 10.0])\n\nlog_v = v.log()\nlog2_v = v.log2()\nlog10_v = v.log10()\nlog1p_v = v.log1p()\n\nlog_v, log2_v, log10_v, log1p_v\n",
   "id": "776b82d24fd8481d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`log1p` and `expm1` are numerically stable for small values and often used in loss functions and probability computations.",
   "id": "1f820a3121194bec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "w = pm.Tensor([-3.0, -1.5, 2.0])\n\nabs_w = w.absolute()\nabs_alias = w.abs()\n\nabs_w, abs_alias\n",
   "id": "b7de785b3a754ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`absolute()` and `abs()` are equivalent; `abs()` mirrors NumPy's naming.",
   "id": "8c3a9ea26eb44ec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5) Comparisons and boolean masks\n\nComparisons are the gatekeepers of math and ML: they decide which elements pass a threshold, which errors are large, or which predictions are correct. There are two flavors in PixelPrism:",
   "id": "5eeab288b5e643ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Python operators like `==` and `<` return a single `bool`. In this implementation `==`, `!=`, `<=`, and `>=` require *all* elements to satisfy the comparison, while `<` and `>` return `True` if *any* element satisfies it.\n\nTensor methods like `equal()` and `less()` return a **Tensor of booleans**, perfect for masks.",
   "id": "43f2a381816c42cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a = pm.Tensor([1.0, 2.0, 3.0])\nb = pm.Tensor([1.0, 0.0, 3.0])\n\nop_eq = a == b\nop_ne = a != b\nop_lt = a < b\nop_le = a <= b\nop_gt = a > b\nop_ge = a >= b\n\nop_eq, op_ne, op_lt, op_le, op_gt, op_ge\n",
   "id": "6d1eeb82cc644536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The operator comparisons return `bool` values (useful for quick checks), while method comparisons return elementwise masks.",
   "id": "3f5e7314e6c04598"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mask_eq = a.equal(b)\nmask_ne = a.not_equal(b)\nmask_lt = a.less(b)\nmask_le = a.less_equal(b)\nmask_gt = a.greater(b)\nmask_ge = a.greater_equal(b)\n\nmask_eq, mask_ne, mask_lt, mask_le, mask_gt, mask_ge\n",
   "id": "b615e1683c654ff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These masks are the building blocks of ML logic: use them to select values, build losses, or filter data.",
   "id": "70fb3bd1fe2e4857"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6) Trigonometry and hyperbolic friends\n\nTrigonometry powers rotations, waves, and periodic signals. In ML it pops up in positional encodings, phase features, and Fourier-like representations. PixelPrism follows NumPy: **angles are in radians** by default.",
   "id": "f322eee82752490b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "angles = pm.Tensor([0.0, np.pi / 6, np.pi / 2, np.pi])\n\nsin_a = angles.sin()\ncos_a = angles.cos()\ntan_a = angles.tan()\n\nsin_a, cos_a, tan_a\n",
   "id": "93f728b6179c4e2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inverse trig functions need values in their valid domains (e.g., `arcsin`/`arccos` for inputs in [-1, 1]).",
   "id": "2328b9d9c8754d56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_unit = pm.Tensor([-1.0, -0.5, 0.0, 0.5, 1.0])\n\nasin_x = x_unit.arcsin()\nacos_x = x_unit.arccos()\natan_x = x_unit.arctan()\n\nasin_x, acos_x, atan_x\n",
   "id": "462011099a3743f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`arctan2` is the angle of a 2D vector (y, x). It's great for turning coordinates into directions.",
   "id": "76a4cfc6ec664fe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = pm.Tensor([0.0, 1.0, 1.0])\nx = pm.Tensor([1.0, 1.0, -1.0])\n\nangles_2d = y.arctan2(x)\nangles_2d\n",
   "id": "58ecf9e3df784df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hyperbolic functions appear in activation functions (think `tanh`) and in smooth feature transforms.",
   "id": "45ce61c0190d46bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "h = pm.Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n\nsinh_h = h.sinh()\ncosh_h = h.cosh()\ntanh_h = h.tanh()\n\nsinh_h, cosh_h, tanh_h\n",
   "id": "2594a0f591b44f4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inverse hyperbolic functions have their own valid domains; keep inputs in safe ranges when exploring.",
   "id": "322ff8b0cab8459b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "h_small = pm.Tensor([-0.9, 0.0, 0.9])\n\nasinh_h = h_small.arcsinh()\natanh_h = h_small.arctanh()\n\nasinh_h, atanh_h\n",
   "id": "33fc3d52b4dc4c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "h_pos = pm.Tensor([1.0, 2.0, 5.0])\n\nacosh_h = h_pos.arccosh()\nacosh_h\n",
   "id": "4722147aa8b046c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Need degrees? Convert with `deg2rad()` or `rad2deg()` before or after trig calls.",
   "id": "ff82bcd59c664fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deg = pm.Tensor([0.0, 30.0, 90.0, 180.0])\n\nrad = deg.deg2rad()\nback_to_deg = rad.rad2deg()\n\nrad, back_to_deg\n",
   "id": "0312b9678c5c4edc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rendering Tensors with Latex",
   "id": "c7f1ec3aa2150e3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pixelprism.math.render.latex import to_latex\n",
    "to_latex(apbpc)"
   ],
   "id": "f7d0d8be7ae0734b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pixelprism.math.render import render_latex\n",
    "latex_image = render_latex(apbpc, output_path=\"latex_image.svg\")\n",
    "latex_image"
   ],
   "id": "b9105be3b2f9bc8a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
